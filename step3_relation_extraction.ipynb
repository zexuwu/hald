{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1a8fd70-62dc-44e9-a5f4-544936c4aec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/hald/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import stanza\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import re\n",
    "from multiprocessing import Process, Manager, Pool\n",
    "import os, time, random\n",
    "import pymongo \n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "491ec003-c1b0-4256-a1d1-1b9d0f1b8eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(username=\"hald\",password=\"XXX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "fc362a6a-db48-4218-8983-ffe1a8ea71f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "now=datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8f2dca7-9cf8-46c0-af3b-ff608d101b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "now_time=now.strftime('%Y_%m_%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2f942af9-73ad-496e-afc3-bf76b8e622e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023_03_14'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9023888f-5ad2-47ea-82df-7f1fc019e35c",
   "metadata": {},
   "source": [
    "now_time='2022_12_09'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0dd2f47e-ca4f-4a9c-9e07-d2523ce7bca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs=['Gene','RNA','Carbohydrate','Lipid','Peptide', 'Pharmaceutical_Preparations', 'Protein', 'Toxin','Mutation', 'Disease']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "50946dcc-3f9b-4855-81a0-025f4115a01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client.hald"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "4ee2a2be-dc83-4f0c-9474-9d1b54fd661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gene_entities=list(db.Gene_raw.find({'addtime':now_time}))\n",
    "RNA_entities=list(db.RNA_raw.find({'addtime':now_time}))\n",
    "Carbohydrate_entities=list(db.Carbohydrate_raw.find({'addtime':now_time}))\n",
    "Lipid_entities=list(db.Lipid_raw.find({'addtime':now_time}))\n",
    "Peptide_entities=list(db.Peptide_raw.find({'addtime':now_time}))\n",
    "Pharmaceutical_Preparations_entities=list(db.Pharmaceutical_Preparations_raw.find({'addtime':now_time}))\n",
    "Protein_entities=list(db.Protein_raw.find({'addtime':now_time}))\n",
    "Toxin_entities=list(db.Toxin_raw.find({'addtime':now_time}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "df37f7c8-a439-4020-81a3-29dca29ee543",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mutation_entities=[]\n",
    "Disease_entities=[]\n",
    "for i in list(db.Mutation_raw.find({'addtime':now_time})):\n",
    "    if i.get('Genomic View'):\n",
    "        Mutation_entities.append(i)\n",
    "for i in list(db.Disease_raw.find({'addtime':now_time})):\n",
    "    if i.get('url'):\n",
    "        Disease_entities.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "31306d4b-4768-44de-adf4-565124f6c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "names=locals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "5c2c4827-efaf-43d3-987a-3aed43c59e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196\n",
      "46\n",
      "239\n",
      "273\n",
      "67\n",
      "2\n",
      "29\n",
      "1\n",
      "106\n",
      "16323\n"
     ]
    }
   ],
   "source": [
    "for i in pairs:\n",
    "    print(len(names[i+'_entities']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "e537f307-f9ba-4354-90ee-ec745816788f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in pairs:\n",
    "    names[j+'_dict']={}\n",
    "    for i in names[j+'_entities']:\n",
    "        names[j+'_dict'][i.get('PMID')]=names[j+'_dict'].get(i.get('PMID'),[])+[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "8487ff40-ed9f-4f77-9f0d-7b982977a4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_relation_dict={}\n",
    "for j in range(len(pairs)-1):\n",
    "    for i in range(j+1,len(pairs)):\n",
    "        for k in list(set(names[pairs[j]+'_dict'].keys()).intersection(set(names[pairs[i]+'_dict'].keys()))):\n",
    "            for l in names[pairs[j]+'_dict'].get(k):\n",
    "                for m in names[pairs[i]+'_dict'].get(k):\n",
    "                    if l.get('sentence')==m.get('sentence') and l.get('entity')!=m.get('entity'):\n",
    "                        diff_relation_dict[l.get('sentence')]=diff_relation_dict.get(l.get('sentence'),[])+[{'PMID':l.get('PMID'),\n",
    "                                                                                                  'entity1':l.get('entity'),\n",
    "                                                                                                   'entity2':m.get('entity'),\n",
    "                                                                                                   'sentence':l.get('sentence'),\n",
    "                                                                                                   'AB':l.get('AB'),\n",
    "                                                                                                   'target1':l.get('target'),\n",
    "                                                                                                    'target1_type':l.get('type'),\n",
    "                                                                                                   'target2':m.get('target'),\n",
    "                                                                                                    'target2_type':m.get('type'),\n",
    "                                                                                                   'PMID':l.get('PMID'),\n",
    "                                                                                                  }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "f9b257bb-dad1-4ebf-8d29-d16147177c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_relation_dict={}\n",
    "for j in range(len(pairs)):\n",
    "    for k in names[pairs[j]+'_dict'].values():\n",
    "        for l in range(len(k)-1):\n",
    "            for m in range(l+1,len(k)):\n",
    "                if k[l].get('sentence')==k[m].get('sentence'):\n",
    "                    if k[l].get('entity')!=k[m].get('entity'):\n",
    "                        same_relation_dict[k[l].get('sentence')]=same_relation_dict.get(k[l].get('sentence'),[])+[{'PMID':k[l].get('PMID'),\n",
    "                                                                                                  'entity1':k[l].get('entity'),\n",
    "                                                                                                   'entity2':k[m].get('entity'),\n",
    "                                                                                                   'sentence':k[l].get('sentence'),\n",
    "                                                                                                   'AB':k[l].get('AB'),\n",
    "                                                                                                   'target1':k[l].get('target'),\n",
    "                                                                                                    'target1_type':k[l].get('type'),\n",
    "                                                                                                   'target2':k[m].get('target'),\n",
    "                                                                                                    'target2_type':k[l].get('type'),\n",
    "                                                                                                   'PMID':k[l].get('PMID'),\n",
    "                                                                                                  }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "f41b4a54-9a74-4461-9a95-e1348235d4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3188\n",
      "731\n",
      "271\n"
     ]
    }
   ],
   "source": [
    "print(len(same_relation_dict))\n",
    "print(len(diff_relation_dict))\n",
    "print(len(list(set(list(same_relation_dict.keys())).intersection(set(list(diff_relation_dict.keys()))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "dbd3342e-e8f9-4a32-8137-88a565984cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_dict={}\n",
    "for i in same_relation_dict.keys():\n",
    "    if diff_relation_dict.get(i):\n",
    "        relation_dict[i]=same_relation_dict.get(i)+diff_relation_dict.get(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "86c1cadd-1677-4c93-9010-9db21b0c966c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3188\n",
      "3648\n"
     ]
    }
   ],
   "source": [
    "for i in same_relation_dict.keys():\n",
    "    if not relation_dict.get(i):\n",
    "        relation_dict[i]=same_relation_dict.get(i)\n",
    "print(len(relation_dict))\n",
    "for i in diff_relation_dict.keys():\n",
    "    if not relation_dict.get(i):\n",
    "        relation_dict[i]=diff_relation_dict.get(i)\n",
    "print(len(relation_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c04584-5af6-4c86-b707-092071032539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9444c758-5248-4a3d-aff1-e952ee8f7223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2aac8d38-5d06-4d06-9043-ed0705abf934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('results/step3/openie/filelist/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2511a9a-d4c6-4c38-96db-35787a4ef3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=1\n",
    "for i in relation_dict.keys():\n",
    "    f=open('results/step3/openie/filelist/sentence'+str(ind)+'.txt','w')\n",
    "    f.write(i)\n",
    "    f.close()\n",
    "    ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5254764a-7500-4934-8f98-e5364ad89f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f=open('results/step3/openie/filelist.txt','a')\n",
    "for i in range(1,len(relation_dict)+1):\n",
    "    f.write('filelist/sentence'+str(i)+'.txt'+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa569be-4ab0-45fe-8d08-199943f66368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a895362a-5548-42b3-8883-e2ee8a961d12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f25831-a93c-44ce-81ef-bb72c4a102cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84eb6863-34ca-4d88-a488-13c53347d835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3648"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75551ce2-62f4-4985-aa54-ccaaeff70800",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=list(relation_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19fcf7d-9f4c-49ea-9413-268268bd8510",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c875bb14-50b4-402a-956c-0a88924f8d36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7cf356-9df9-47ed-adea-171e821735b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8038659c-d703-4889-90fb-3318ba92d4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('results/step3/openie/files/')\n",
    "k=1\n",
    "for i in range(0,len(sentences),len(sentences)//20):\n",
    "    pickle.dump(sentences[i:i+len(sentences)//20],open('results/step3/openie/files/openie'+str(k)+'.pkl','wb'))\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55decc98-3523-49c8-9979-f5d8ddfbd33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('results/step3/rules/files/')\n",
    "k=1\n",
    "for i in range(0,len(sentences),len(sentences)//20):\n",
    "    f=open('results/step3/rules/files/rules'+str(k)+'.py','w')\n",
    "    f.write('''\n",
    "import stanza\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "nlp = stanza.Pipeline('en', package='genia',download_method=None)\n",
    "import pickle\n",
    "raw=pickle.load(open('../../openie/files/openie'''+str(k)+'''.pkl','rb'))\n",
    "import re \n",
    "\n",
    "def nlpSentence(i,sentence2words,sentence2relations,sentence2verblist,sentence2verbtype,sentence2adv):\n",
    "    sentence=i\n",
    "    doc=nlp(sentence)\n",
    "    words=[]\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            words.append(word.text)\n",
    "    relations=[]\n",
    "    verblist=[]\n",
    "    verbprototype=[]\n",
    "    adv=[]\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            relations.append([word.text,words[int(word.head)-1],int(word.id),int(word.head),word.deprel,word.xpos])\n",
    "            if word.xpos in ['VB','VBD','VBG','VBN', 'VBP', 'VBZ']:\n",
    "                verblist.append({word.text:word.xpos})\n",
    "                verbprototype.append({word.text:word.lemma})\n",
    "            if word.xpos in ['RB','RBR','RBS']:\n",
    "                adv.append({word.text:word.xpos})\n",
    "    sentence2words[i]=words\n",
    "    sentence2relations[i]=relations\n",
    "    sentence2verblist[i]=verblist\n",
    "    sentence2verbprototype[i]=verbprototype\n",
    "    sentence2adv[i]=adv\n",
    "\n",
    "\n",
    "start=datetime.datetime.now()\n",
    "print('Parent process %s.' % os.getpid())\n",
    "sentence2words={}\n",
    "sentence2relations={}\n",
    "sentence2verblist={}\n",
    "sentence2verbprototype={}\n",
    "sentence2adv={}\n",
    "for i in raw:\n",
    "    nlpSentence(i,sentence2words,sentence2relations,sentence2verblist,sentence2verbprototype,sentence2adv)\n",
    "pickle.dump(sentence2words,open('sentence2words'''+str(k)+'''.pkl','wb'))\n",
    "pickle.dump(sentence2relations,open('sentence2relations'''+str(k)+'''.pkl','wb'))\n",
    "pickle.dump(sentence2verblist,open('sentence2verblist'''+str(k)+'''.pkl','wb'))\n",
    "pickle.dump(sentence2verbprototype,open('sentence2verbprototype'''+str(k)+'''.pkl','wb'))\n",
    "pickle.dump(sentence2adv,open('sentence2adv'''+str(k)+'''.pkl','wb'))\n",
    "print('Waiting for all subprocesses done...')\n",
    "\n",
    "end=datetime.datetime.now()\n",
    "print(\"The running time is \"+str((end-start).seconds)+\"s\")  \n",
    "    ''')\n",
    "    f.close()\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d4c833ef-498e-49d2-b5cc-a8cd16c45865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/hald_autoupdate/2023_03_13'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7c974f-03c3-4baa-840e-88cbe91dc273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shell\n",
    "# cd openie\n",
    "# nohup java -mx500g -cp ../../../../stanza_corenlp/stanford-corenlp-4.3.2.jar:../../../../stanza_corenlp/stanford-corenlp-4.3.2-models.jar:../../../../stanza_corenlp/CoreNLP-to-HTML.xsl:../../../../slf4j-api.jar:../../../../stanza_corenlp/slf4j-simple.jar edu.stanford.nlp.naturalli.OpenIE -format reverb -filelist ./filelist.txt -output ./openie.txt > ./openie.log &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e7d76fa-9657-409f-99db-7bbf3c0adb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shell \n",
    "# cd rules/files\n",
    "# for i in {1..21}; do echo \"nohup python rules${i}.py > rules.log &\" | bash; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e810049e-8964-40bc-a86f-fa1dee22fa01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484e2d57-4bb4-4589-a739-310cd2ff697e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152a60ba-0266-4d21-8003-43269a23643a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7dc95d9c-76f0-4c8d-b6cb-03d8fdd5c76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "19218ca8-b103-4822-ba93-4663dd79d69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "openie_table=pd.read_table('results/step3/openie/openie.txt',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "cc7ed1c3-0f04-45b4-a9ac-088ad1371e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>filelist/sentence19.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>DDIT3 knockdown</td>\n",
       "      <td>is</td>\n",
       "      <td>able</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Moreover , DDIT3 knockdown in CD34 + cells fro...</td>\n",
       "      <td>RB , NN NN IN NN SYM NNS IN NNP NNS IN NN VBZ ...</td>\n",
       "      <td>ddit3 knockdown</td>\n",
       "      <td>be</td>\n",
       "      <td>able</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>filelist/sentence19.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>DDIT3 knockdown</td>\n",
       "      <td>is in</td>\n",
       "      <td>CD34 cells</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Moreover , DDIT3 knockdown in CD34 + cells fro...</td>\n",
       "      <td>RB , NN NN IN NN SYM NNS IN NNP NNS IN NN VBZ ...</td>\n",
       "      <td>ddit3 knockdown</td>\n",
       "      <td>be in</td>\n",
       "      <td>cd34 cell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>filelist/sentence19.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>MDS patients</td>\n",
       "      <td>is with</td>\n",
       "      <td>anemia</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Moreover , DDIT3 knockdown in CD34 + cells fro...</td>\n",
       "      <td>RB , NN NN IN NN SYM NNS IN NNP NNS IN NN VBZ ...</td>\n",
       "      <td>MDS patient</td>\n",
       "      <td>be with</td>\n",
       "      <td>anemia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>filelist/sentence19.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>DDIT3 knockdown</td>\n",
       "      <td>restore</td>\n",
       "      <td>erythropoiesis</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Moreover , DDIT3 knockdown in CD34 + cells fro...</td>\n",
       "      <td>RB , NN NN IN NN SYM NNS IN NNP NNS IN NN VBZ ...</td>\n",
       "      <td>ddit3 knockdown</td>\n",
       "      <td>restore</td>\n",
       "      <td>erythropoiesis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>filelist/sentence19.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>DDIT3 knockdown</td>\n",
       "      <td>is</td>\n",
       "      <td>Moreover able</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Moreover , DDIT3 knockdown in CD34 + cells fro...</td>\n",
       "      <td>RB , NN NN IN NN SYM NNS IN NNP NNS IN NN VBZ ...</td>\n",
       "      <td>ddit3 knockdown</td>\n",
       "      <td>be</td>\n",
       "      <td>moreover able</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        0   1                2        3               4   5   \\\n",
       "0  filelist/sentence19.txt   0  DDIT3 knockdown       is            able   2   \n",
       "1  filelist/sentence19.txt   0  DDIT3 knockdown    is in      CD34 cells   2   \n",
       "2  filelist/sentence19.txt   0     MDS patients  is with          anemia   9   \n",
       "3  filelist/sentence19.txt   0  DDIT3 knockdown  restore  erythropoiesis   2   \n",
       "4  filelist/sentence19.txt   0  DDIT3 knockdown       is   Moreover able   2   \n",
       "\n",
       "   6   7   8   9   10   11                                                 12  \\\n",
       "0   4  13  14  14  15  1.0  Moreover , DDIT3 knockdown in CD34 + cells fro...   \n",
       "1   4   4   5   5   8  1.0  Moreover , DDIT3 knockdown in CD34 + cells fro...   \n",
       "2  11  11  12  12  13  1.0  Moreover , DDIT3 knockdown in CD34 + cells fro...   \n",
       "3   4  16  17  17  18  1.0  Moreover , DDIT3 knockdown in CD34 + cells fro...   \n",
       "4   4  13  14   0  15  1.0  Moreover , DDIT3 knockdown in CD34 + cells fro...   \n",
       "\n",
       "                                                  13               14  \\\n",
       "0  RB , NN NN IN NN SYM NNS IN NNP NNS IN NN VBZ ...  ddit3 knockdown   \n",
       "1  RB , NN NN IN NN SYM NNS IN NNP NNS IN NN VBZ ...  ddit3 knockdown   \n",
       "2  RB , NN NN IN NN SYM NNS IN NNP NNS IN NN VBZ ...      MDS patient   \n",
       "3  RB , NN NN IN NN SYM NNS IN NNP NNS IN NN VBZ ...  ddit3 knockdown   \n",
       "4  RB , NN NN IN NN SYM NNS IN NNP NNS IN NN VBZ ...  ddit3 knockdown   \n",
       "\n",
       "        15              16  \n",
       "0       be            able  \n",
       "1    be in       cd34 cell  \n",
       "2  be with          anemia  \n",
       "3  restore  erythropoiesis  \n",
       "4       be   moreover able  "
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openie_table.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f126fbbf-b1ac-443f-9ea4-5b501654a6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54068"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3770378\n",
    "len(openie_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "feb785ec-5d25-49e6-a781-f8fd3ea7b631",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=openie_table.loc[:,[0,2,3,4,12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "44a5c1e7-9e2b-4c5e-9c8f-8c323a9a0743",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=list(relation_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0dc9a59d-3956-4858-b6b1-515be2bf51c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "be=['be','am','is','are','was','were','have been','has been']\n",
    "bedict={}\n",
    "for i in be:\n",
    "    bedict[i]='be'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "102b3e86-57ac-40fa-9c21-936f8330e807",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_pairs=['sentence2adv','sentence2relations','sentence2verblist','sentence2verbprototype','sentence2words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8eda8337-87e4-41e4-9e7f-3bf095a6936b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/hald_autoupdate/2023_03_13'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7cec02c6-1a29-4cef-bf92-8416629a3658",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in rules_pairs:\n",
    "    names[i]={}\n",
    "    for k in ['results/step3/rules/files/'+i+str(j)+'.pkl' for j in range(1,22)]:\n",
    "        names[i].update(pickle.load(open(k,'rb')))\n",
    "    # pickle.dump(names[i],open('files/'+i+'.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "b88f59b6-1ad5-4955-b4ea-939141710eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "finallist=[]\n",
    "for line in range(len(df)):\n",
    "    i=sentences[int(df.iloc[line,0][17:-4])-1]\n",
    "    verblist=sentence2verblist.get(i)\n",
    "    verbrelation=''\n",
    "    real_verb=''\n",
    "    verbdict={}\n",
    "    verbs_split=df.loc[line,3].split(' ')\n",
    "    for verb in verblist:\n",
    "        verbdict.update(verb)\n",
    "    # if len(verbs.split(' '))==1:\n",
    "    #     real_verb=verbs.split(' ')[0]\n",
    "    # else:\n",
    "\n",
    "    for indexx,one in enumerate(verbs_split):\n",
    "        if verbdict.get(one):\n",
    "            verbrelation=one\n",
    "            relations=sentence2relations.get(i)\n",
    "            relations_display=[relation[-2] for relation in relations]\n",
    "            verbprototype=sentence2verbprototype.get(i)\n",
    "            verbprototype_dict={}\n",
    "            for verbp in verbprototype:\n",
    "                verbprototype_dict.update(verbp)\n",
    "            for index_passive,passive in enumerate(relations_display):\n",
    "                if passive=='aux:pass':\n",
    "                    if relations[index_passive][1]==verbrelation:\n",
    "                        verb_prototype=verbrelation\n",
    "                        break\n",
    "                elif passive=='acl' and relations[index_passive][-1]=='VBN':\n",
    "                    if relations[index_passive][0]==verbrelation:\n",
    "                        verb_prototype=verbrelation\n",
    "                        break\n",
    "            else:\n",
    "                for v in verbprototype:\n",
    "                    if v.get(verbrelation):\n",
    "                        verb_prototype=v.get(verbrelation)\n",
    "                        break\n",
    "            verbs_split[indexx]=verb_prototype\n",
    "        elif bedict.get(one):\n",
    "            verbs_split[indexx]=bedict.get(one)\n",
    "    if bedict.get(' '.join(verbs_split[:2])):\n",
    "        final=' '.join(['be']+verbs_split[2:])\n",
    "    else:\n",
    "        final=' '.join(verbs_split)\n",
    "    finallist.append(final)\n",
    "df[17]=finallist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a4cce1-1aff-43d0-b972-c68dc8cca9d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "73bf420d-b083-4f93-9334-bd2dd4f4aace",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=1\n",
    "for i in relation_dict.keys():\n",
    "    names['sentence'+str(ind)]=i\n",
    "    ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "97efa3ca-ff16-4b6e-83a0-2aa8af379f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d30b7954-ec30-48e0-bda0-ce2990e768a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_dict_copy=copy.deepcopy(relation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "7d51bfe4-1a83-4040-82bd-fbca19ec6439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "suc=[]\n",
    "for i in range(1,len(relation_dict_copy)+1):\n",
    "    if relation_dict_copy.get(names[df.iloc[i-1,0][9:-4]]):\n",
    "        openie=[]\n",
    "        for j in relation_dict_copy.get(names[df.iloc[i-1,0][9:-4]]):\n",
    "            j1=j.get('target1').translate(str.maketrans({\"-\":  r\"\\-\",\"\\\\\": r\"\\\\\", \"^\":  r\"\\^\",\"$\":  r\"\\$\",\"*\":  r\"\\*\",\".\":  r\"\\.\",\"(\":  r\"\\(\",\")\":  r\"\\)\",\"+\":  r\"\\+\",\"[\":  r\"\\[\",\"]\":  r\"\\]\",\n",
    "                      \"{\":  r\"\\{\",\"}\":  r\"\\}\",\"|\":  r\"\\|\",\"?\":  r\"\\?\"}))\n",
    "            j2=j.get('target2').translate(str.maketrans({\"-\":  r\"\\-\",\"\\\\\": r\"\\\\\", \"^\":  r\"\\^\",\"$\":  r\"\\$\",\"*\":  r\"\\*\",\".\":  r\"\\.\",\"(\":  r\"\\(\",\")\":  r\"\\)\",\"+\":  r\"\\+\",\"[\":  r\"\\[\",\"]\":  r\"\\]\",\n",
    "                      \"{\":  r\"\\{\",\"}\":  r\"\\}\",\"|\":  r\"\\|\",\"?\":  r\"\\?\"}))\n",
    "            if re.search(j1,df.iloc[i-1,1]) and re.search(j2,df.iloc[i-1,3]):\n",
    "                j['openie']=j.get('openie',[])+[[j.get('target1'),df.iloc[i-1,-1],j.get('target2')]]\n",
    "                openie.append([j.get('target1'),df.iloc[i-1,-1],j.get('target2')])\n",
    "            elif re.search(j2,df.iloc[i-1,1]) and re.search(j1,df.iloc[i-1,3]):\n",
    "                j['openie']=j.get('openie',[])+[[j.get('target2'),df.iloc[i-1,-1],j.get('target1')]]\n",
    "                openie.append([j.get('target2'),df.iloc[i-1,-1],j.get('target1')]) \n",
    "        if not openie:\n",
    "            suc.append(names[df.iloc[i-1,0][9:-4]])\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "70d7b0ec-39b7-4fbe-90f7-beed4f3c42a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "error=[]\n",
    "for i in range(1,len(relation_dict_copy)+1):\n",
    "    if not relation_dict_copy.get(names[df.iloc[i-1,0][9:-4]]):\n",
    "        error.append(names[df.iloc[i-1,0][9:-4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "7658e8f2-c8fc-4f55-8f04-a5f24d85b65a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3327"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(suc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "2319ff5b-aa21-46b7-96f3-ebbcae2190bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3648"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relation_dict_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "00936837-a355-4516-89e3-723e0e73889a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "440aacb0-6152-4027-b099-3dece5eb73d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.structured_prediction\n",
    "PREDICTOR = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/openie-model.2020.03.26.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "271e9675-cdf6-4be5-881f-a56d80936640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean(sent):\n",
    "    return re.sub(' +', ' ', sent.replace('\\r', ' ').replace('\\n', ' '))\n",
    "\n",
    "\n",
    "def get_tags_allen(sent):\n",
    "    sent = sent.replace('[', '(').replace(']', ')')\n",
    "    res = PREDICTOR.predict(\n",
    "      sentence=sent\n",
    "    )\n",
    "    return res\n",
    "\n",
    "\n",
    "def parse_allen(ss):\n",
    "    words = ss['words']\n",
    "    \n",
    "    res = []\n",
    "    for v in ss['verbs']:\n",
    "        tags = v['tags']\n",
    "\n",
    "        d = {}\n",
    "        for i in range(len(words)):\n",
    "            t = tags[i].replace('I-', '').replace('B-', '')\n",
    "            w = words[i]\n",
    "            if t in d:\n",
    "                d[t] += ' '+ w\n",
    "            else:\n",
    "                d[t] = w\n",
    "        res.append(d)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23ccc86-3ee3-4a16-afd9-0514c0425ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2allennlp={}\n",
    "for i in relation_dict_copy.keys():\n",
    "    res=get_tags_allen(i)\n",
    "    ress=parse_allen(res)\n",
    "    sentence2allennlp[i]=ress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2175b34e-7f25-483b-9908-95cdbd551342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "for i,j in relation_dict_copy.items():\n",
    "    for k in j:\n",
    "        k1=k.get('target1').translate(str.maketrans({\"-\":  r\"\\-\",\"\\\\\": r\"\\\\\", \"^\":  r\"\\^\",\"$\":  r\"\\$\",\"*\":  r\"\\*\",\".\":  r\"\\.\",\"(\":  r\"\\(\",\")\":  r\"\\)\",\"+\":  r\"\\+\",\"[\":  r\"\\[\",\"]\":  r\"\\]\",\n",
    "                  \"{\":  r\"\\{\",\"}\":  r\"\\}\",\"|\":  r\"\\|\",\"?\":  r\"\\?\"}))\n",
    "        k2=k.get('target2').translate(str.maketrans({\"-\":  r\"\\-\",\"\\\\\": r\"\\\\\", \"^\":  r\"\\^\",\"$\":  r\"\\$\",\"*\":  r\"\\*\",\".\":  r\"\\.\",\"(\":  r\"\\(\",\")\":  r\"\\)\",\"+\":  r\"\\+\",\"[\":  r\"\\[\",\"]\":  r\"\\]\",\n",
    "                  \"{\":  r\"\\{\",\"}\":  r\"\\}\",\"|\":  r\"\\|\",\"?\":  r\"\\?\"}))\n",
    "        for l in sentence2allennlp.get(i):\n",
    "            if l.get('ARG1') and l.get('ARG2') and l.get('V'):\n",
    "                if re.search(k1,l.get('ARG1')) and re.search(k2,l.get('ARG2')):\n",
    "                    k['allennlp']=k.get('allennlp',[])+[[k.get('target1'), l.get('V') ,k.get('target2')]]\n",
    "                elif re.search(k2,l.get('ARG1')) and re.search(k1,l.get('ARG2')):\n",
    "                    k['allennlp']=k.get('allennlp',[])+[[k.get('target2'), l.get('V') ,k.get('target1')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c72e5b2-6b41-4e2f-a77f-d8f2cc7fb050",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in relation_dict_copy.items():\n",
    "    for k in j:\n",
    "        if k.get('allennlp'):\n",
    "            print(k)\n",
    "            break\n",
    "    else:\n",
    "        continue\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70db993-e2b3-4120-9190-5745fbf99501",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentence2adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc63d705-773f-438c-b944-718bf1b2c8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "62401dbd-f972-4527-a77a-92a3e1ca54ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/hald_autoupdate/2023_03_13'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dfb7f62f-02bb-4eec-83bd-6f4897089c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "negativewords=['no','not','never','hardly','barely','scarcely','rarely','few','little','seldom','neither','nor','negatively']\n",
    "negativewords_dict={}\n",
    "for i in negativewords:\n",
    "    negativewords_dict[i]=i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd34f2e3-70e4-4401-b57c-6eedeec9bc14",
   "metadata": {},
   "source": [
    "for i,j in relation_dict_copy.items():\n",
    "    for k in j:\n",
    "        if k.get('openie'):\n",
    "            del k['openie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "4accfd64-869b-43c2-b86e-0424190d5ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent process 24627.\n",
      "Waiting for all subprocesses done...\n",
      "All subprocesses done.\n",
      "The running time is 31s\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import mpl\n",
    "import re\n",
    "from multiprocessing import Process, Manager, Pool\n",
    "import os, time, random\n",
    "def work(i,j,triple,abnormal, negative_sentences,networkx_raw):\n",
    "    adv=sentence2adv.get(i)\n",
    "    words=sentence2words.get(i)\n",
    "\n",
    "    sentence=i\n",
    "    # length=len(sentence2words.get(i))\n",
    "\n",
    "    relations=sentence2relations.get(i)\n",
    "    relations_display=[relation[-2] for relation in relations]\n",
    "    verblist=sentence2verblist.get(i)\n",
    "    verbprototype=sentence2verbprototype.get(i)\n",
    "    verbprototype_dict={}\n",
    "    for verbp in verbprototype:\n",
    "        verbprototype_dict.update(verbp)\n",
    "    # for k in range(length):\n",
    "    G = nx.Graph()  # 建立无向图G\n",
    "\n",
    "    # 添加节点\n",
    "    for word in words:\n",
    "        G.add_node(word)\n",
    "\n",
    "    G.add_node('root')\n",
    "\n",
    "    # 添加边\n",
    "    for r in range(len(relations)):\n",
    "        if relations[r][4]=='root':\n",
    "            G.add_edge(relations[r][0], 'root',labels=relations[r][4])\n",
    "        else:\n",
    "            G.add_edge(relations[r][0], relations[r][1],labels=relations[r][4])\n",
    "\n",
    "    for k in j:\n",
    "        if verblist:\n",
    "            k1=k.get('target1').translate(str.maketrans({\"-\":  r\"\\-\",\"\\\\\": r\"\\\\\", \"^\":  r\"\\^\",\"$\":  r\"\\$\",\"*\":  r\"\\*\",\".\":  r\"\\.\",\"(\":  r\"\\(\",\")\":  r\"\\)\",\"+\":  r\"\\+\",\"[\":  r\"\\[\",\"]\":  r\"\\]\",\n",
    "                                      \"{\":  r\"\\{\",\"}\":  r\"\\}\",\"|\":  r\"\\|\",\"?\":  r\"\\?\"}))\n",
    "            k2=k.get('target2').translate(str.maketrans({\"-\":  r\"\\-\",\"\\\\\": r\"\\\\\", \"^\":  r\"\\^\",\"$\":  r\"\\$\",\"*\":  r\"\\*\",\".\":  r\"\\.\",\"(\":  r\"\\(\",\")\":  r\"\\)\",\"+\":  r\"\\+\",\"[\":  r\"\\[\",\"]\":  r\"\\]\",\n",
    "                                      \"{\":  r\"\\{\",\"}\":  r\"\\}\",\"|\":  r\"\\|\",\"?\":  r\"\\?\"}))\n",
    "            if re.search(k1,sentence) and re.search(k2,sentence):\n",
    "                worddict={}\n",
    "                for word in words:\n",
    "                    worddict[word]=word\n",
    "                if re.search(k1,sentence).span()[0] < re.search(k2,sentence).span()[0]:\n",
    "                    k1_word=k.get('target1')\n",
    "                    k2_word=k.get('target2')\n",
    "                    if not worddict.get(k1_word):\n",
    "                        for word in words:\n",
    "                            if re.search(r'-',word) and re.search(k1,word):\n",
    "                                k1_word=word\n",
    "                    if not worddict.get(k2_word):\n",
    "                        for word in words:\n",
    "                            if re.search(r'-',word) and re.search(k2,word):\n",
    "                                k2_word=word\n",
    "                    try:\n",
    "                        path=nx.shortest_path(G, source=re.split('[ -]',k1_word)[-1], target=re.split('[ -]',k2_word)[-1])\n",
    "                    except:\n",
    "                        abnormal.append(i)\n",
    "                    else:\n",
    "                        verbdict={}\n",
    "                        for verb in verblist:\n",
    "                            verbdict.update(verb)\n",
    "                        verbrelation=''\n",
    "                        for one in path:\n",
    "                            if verbdict.get(one):\n",
    "                                verbrelation=one\n",
    "\n",
    "                        # verbprototype[verblist.index(verb)]    \n",
    "                        if verbrelation :\n",
    "                            negative=False\n",
    "                            for index_passive,passive in enumerate(relations_display):\n",
    "                                if passive=='aux:pass':\n",
    "                                    if relations[index_passive][1]==verbrelation:\n",
    "                                        verb_prototype=verbrelation\n",
    "                                        break\n",
    "                                elif passive=='acl' and relations[index_passive][-1]=='VBN':\n",
    "                                    if relations[index_passive][0]==verbrelation:\n",
    "                                        verb_prototype=verbrelation\n",
    "                                        break\n",
    "                            else:\n",
    "                                for v in verbprototype:\n",
    "                                    if v.get(verbrelation):\n",
    "                                        verb_prototype=v.get(verbrelation)\n",
    "                                        break\n",
    "                            if verb_prototype:\n",
    "                                for relation in relations:\n",
    "                                    if relation[1]==verbrelation and negativewords_dict.get(relation[0]):\n",
    "                                        negative=True\n",
    "                                if not negative:\n",
    "                                    triple.append({'triple':[k.get('target1'),verb_prototype,k.get('target2')],\\\n",
    "                                                        'PMID':k.get('PMID'), 'sentence':sentence,'AB':k.get('AB'),\n",
    "                                                   'former_target':k.get('target1'),'former_entity':k.get('entity1'),'former_entity_type':k.get('target1_type'),\n",
    "                                                   'verb':verbrelation,'verbprototype':verbprototype_dict.get(verbrelation),'latter_target':k.get('target2'),\n",
    "                                                   'latter_entity':k.get('entity2'),'latter_entity_type':k.get('target2_type')})\n",
    "                                    k['networkx']=[k.get('target1'),verb_prototype,k.get('target2')]\n",
    "                                else:\n",
    "                                    negative_sentences.append(i)\n",
    "                else:\n",
    "                    k1_word=k.get('target1')\n",
    "                    k2_word=k.get('target2')\n",
    "                    if not worddict.get(k1_word):\n",
    "                        for word in words:\n",
    "                            if re.search(r'-',word) and re.search(k1,word):\n",
    "                                k1_word=word\n",
    "                    if not worddict.get(k2_word):\n",
    "                        for word in words:\n",
    "                            if re.search(r'-',word) and re.search(k2,word):\n",
    "                                k2_word=word\n",
    "                    try:\n",
    "                        path=nx.shortest_path(G, source=re.split('[ -]',k2_word)[-1], target=re.split('[ -]',k1_word)[-1])\n",
    "                    except:\n",
    "                        abnormal.append(i)\n",
    "                    else:\n",
    "                        verbdict={}\n",
    "                        for verb in verblist:\n",
    "                            verbdict.update(verb)\n",
    "                        verbrelation=''\n",
    "                        for one in path:\n",
    "                            if verbdict.get(one):\n",
    "                                verbrelation=one\n",
    "                        # verbprototype[verblist.index(verb)]    \n",
    "                        if verbrelation:\n",
    "                            negative=False\n",
    "                            for index_passive,passive in enumerate(relations_display):\n",
    "                                if passive=='aux:pass':\n",
    "                                    if relations[index_passive][1]==verbrelation:\n",
    "                                        verb_prototype=verbrelation\n",
    "                                        break\n",
    "                                elif passive=='acl' and relations[index_passive][-1]=='VBN':\n",
    "                                    if relations[index_passive][0]==verbrelation:\n",
    "                                        verb_prototype=verbrelation\n",
    "                                        break\n",
    "                            else:\n",
    "                                for v in verbprototype:\n",
    "                                    if v.get(verbrelation):\n",
    "                                        verb_prototype=v.get(verbrelation)\n",
    "                                        break\n",
    "                            if verb_prototype:\n",
    "                                for relation in relations:\n",
    "                                    if relation[1]==verbrelation and negativewords_dict.get(relation[0]):\n",
    "                                        negative=True\n",
    "                                if not negative:\n",
    "                                    triple.append({'triple':[k.get('target2'),verb_prototype,k.get('target1')],\\\n",
    "                                                        'PMID':k.get('PMID'), 'sentence':sentence,'AB':k.get('AB'),\n",
    "                                                   'former_target':k.get('target2'),'former_entity':k.get('entity2'),'former_entity_type':k.get('target2_type'),\n",
    "                                                   'verb':verbrelation,'verbprototype':verbprototype_dict.get(verbrelation),'latter_target':k.get('target1'),\n",
    "                                                   'latter_entity':k.get('entity1'),'latter_entity_type':k.get('target1_type')})\n",
    "                                    k['networkx']=[k.get('target2'),verb_prototype,k.get('target1')]\n",
    "                                else:\n",
    "                                    negative_sentences.append(i)\n",
    "        networkx_raw.append(k)\n",
    "if __name__=='__main__':\n",
    "    start=datetime.datetime.now()\n",
    "    print('Parent process %s.' % os.getpid())\n",
    "    p=Pool(40)\n",
    "    manager=Manager()\n",
    "    triple=manager.list()\n",
    "    abnormal=manager.list()\n",
    "    negative_sentences=manager.list()\n",
    "    networkx_raw=manager.list()\n",
    "    for i,j in relation_dict_copy.items():\n",
    "        p.apply_async(work, args=(i,j,triple,abnormal,negative_sentences,networkx_raw))\n",
    "\n",
    "    print('Waiting for all subprocesses done...')\n",
    "    \n",
    "    p.close()\n",
    "    p.join()\n",
    "    print('All subprocesses done.')\n",
    "\n",
    "    end=datetime.datetime.now()\n",
    "    print(\"The running time is \"+str((end-start).seconds)+\"s\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "6e3344e8-91eb-464e-a2ad-57e1ac974ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "triple_list=list(triple)\n",
    "abnormal_list=list(abnormal)\n",
    "negative_list=list(negative_sentences)\n",
    "networkx_raw_list=list(networkx_raw)\n",
    "networkx_raw_dict={}\n",
    "for i in networkx_raw_list:\n",
    "    networkx_raw_dict[i.get('sentence')]=networkx_raw_dict.get(i.get('sentence'),[])+[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "d5cc028f-1b4a-4bf5-90dd-af404b0b9799",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(triple_list,open('results/step3/triple_list.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "de2953f3-bf0d-46a0-80d6-e70cb4ff72af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(abnormal_list,open('results/step3/abnormal_list.pkl','wb'))\n",
    "pickle.dump(negative_list,open('results/step3/negative_list.pkl','wb'))\n",
    "pickle.dump(networkx_raw_list,open('results/step3/networkx_raw_list.pkl','wb'))\n",
    "pickle.dump(networkx_raw_dict,open('results/step3/networkx_raw_dict.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "0b52906b-0e18-4a24-a1cb-51fa734d495e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_pick_pubtator=pickle.load(open('results/step1/raw_pick_pubtator.pkl','rb'))\n",
    "rawpickwithAB={}\n",
    "for i in raw_pick_pubtator:\n",
    "    rawpickwithAB[i.get('PMID')]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "ba92d711-8208-42ae-8a84-dc64e122c442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PMID': '36845012',\n",
       " 'entity1': 'IER3',\n",
       " 'entity2': 'ARNTL',\n",
       " 'sentence': 'Among the differentially expressed genes, aryl hydrocarbon receptor nuclear translocator-like (ARNTL), BTG antiproliferation factor 2 (BTG2), C-X-C motif chemokine ligand 10 (CXCL10), chitinase 3-like 1 (CHI3L1), immediate early response 3 (IER3), Fos proto-oncogene, AP-1 transcription factor subunit (FOS), and peroxisome proliferative activated receptor, gamma, coactivator 1 alpha (PPARGC1A), mainly involved in the regulation of cell proliferation, metabolism, and inflammation, were also dysregulated in liver tissues suffered from IRI and could form a FOS-centered interaction network.',\n",
       " 'AB': 'Background: With the intensification of population aging, the proportion of aging livers in the donor pool is increasing rapidly. Compared with young livers, aging livers are more susceptible to ischemia-reperfusion injury (IRI) during liver transplantation, which greatly affects the utilization rate of aging livers. The potential risk factors associated with IRI in aging livers have not been fully elucidated. Methods: In this work, five human liver tissue expression profiling datasets (GSE61260, GSE107037, GSE89632, GSE133815, and GSE151648) and a total of 28 young and aging liver tissues of human (N = 20) and mouse (N = 8) were used to screen and verify the potential risk factors associated with aging livers being more prone to IRI. DrugBank Online was used to screen drugs with potential to alleviate IRI in aging livers. Results: The gene expression profile and immune cell composition between young and aging livers had significant differences. Among the differentially expressed genes, aryl hydrocarbon receptor nuclear translocator-like (ARNTL), BTG antiproliferation factor 2 (BTG2), C-X-C motif chemokine ligand 10 (CXCL10), chitinase 3-like 1 (CHI3L1), immediate early response 3 (IER3), Fos proto-oncogene, AP-1 transcription factor subunit (FOS), and peroxisome proliferative activated receptor, gamma, coactivator 1 alpha (PPARGC1A), mainly involved in the regulation of cell proliferation, metabolism, and inflammation, were also dysregulated in liver tissues suffered from IRI and could form a FOS-centered interaction network. Nadroparin was screened out with the potential to target FOS in DrugBank Online. In addition, the proportion of dendritic cells (DCs) was significantly upregulated in aging livers. Conclusions: We combined the expression profiling datasets of liver tissues and samples collected in our hospital for the first time to reveal that the changes in the expression of ARNTL, BTG2, CXCL10, CHI3L1, IER3, FOS, and PPARGC1A and the proportion of dendritic cells may be associated with aging livers being more prone to IRI. Nadroparin may be used to mitigate IRI in aging livers by targeting FOS, and regulation of DC activity may also reduce IRI.',\n",
       " 'target1': 'IER3',\n",
       " 'target1_type': 'Gene',\n",
       " 'target2': 'ARNTL',\n",
       " 'target2_type': 'Gene'}"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "networkx_raw_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "1cd05c1a-8899-4a8e-9ae3-8c627b78f992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3648"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(networkx_raw_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "6b378fed-619d-4988-9cb0-9676ff487162",
   "metadata": {},
   "outputs": [],
   "source": [
    "be=['be','am','is','are','was','were','have been','has been']\n",
    "bedict={}\n",
    "for i in be:\n",
    "    bedict[i]=i\n",
    "can=['can','could','will','would','may','might']\n",
    "candict={}\n",
    "for i in can:\n",
    "    candict[i]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "aa802227-0c1a-41a2-849d-33364e90ef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in networkx_raw_list:\n",
    "    openie_mid=[]\n",
    "    openie_final=[]\n",
    "    if i.get('openie'):\n",
    "        for j in i.get('openie'):\n",
    "            if not bedict.get(j[1]) and not candict.get(j[1]):\n",
    "                openie_mid.append(j)\n",
    "        adv=sentence2adv.get(i.get('sentence'))\n",
    "        adv_dict={}\n",
    "        for l in adv:\n",
    "            adv_dict.update(l)\n",
    "        for k in openie_mid:\n",
    "            for m in k[1].split(' '):\n",
    "                if adv_dict.get(m):\n",
    "                    break\n",
    "            else:\n",
    "                if k not in openie_final:\n",
    "                    openie_final.append(k)\n",
    "    i['openie_final']=openie_final\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "bad4ea4c-d355-4173-8cdd-3e72b2b48989",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in networkx_raw_list:\n",
    "    allennlp_final=[]\n",
    "    if i.get('allennlp'):\n",
    "        for j in i.get('allennlp'):\n",
    "            if not bedict.get(j[1]) and not candict.get(j[1]):\n",
    "                allennlp_final.append(j)\n",
    "    i['allennlp_final']=allennlp_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "74cc04fe-cb9f-45b8-8655-900b80a8794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in networkx_raw_list:\n",
    "    if i.get('networkx'):\n",
    "        if bedict.get(i.get('networkx')[1]):\n",
    "            i['networkx']=''\n",
    "        elif i.get('networkx')[1]=='\"':\n",
    "            i['networkx']=''\n",
    "    else:\n",
    "        i['networkx']=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "3b70e380-ca99-4aeb-b749-49ee188b8749",
   "metadata": {},
   "outputs": [],
   "source": [
    "networkx_raw_final=[]\n",
    "for i in networkx_raw_list:\n",
    "    if i.get('networkx') or i.get('openie_final') or i.get('allennlp_final'):\n",
    "        networkx_raw_final.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "bcfe38dc-1487-436b-a75b-b4c74a44e7e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2042"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(networkx_raw_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "bf155ffe-8edf-4681-93f9-4f87191e26f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in networkx_raw_final:\n",
    "    bidirection=False\n",
    "    if i.get('networkx'):\n",
    "        if i.get('networkx')[1]=='associated' or i.get('networkx')[1]=='associate' :\n",
    "            i['bidirection']=True\n",
    "            continue\n",
    "    if i.get('allennlp'):\n",
    "        for j in i.get('openie_final'):\n",
    "            if j=='associated' or j=='associate' :\n",
    "                bidirection=True\n",
    "                break\n",
    "    if i.get('openie_final'):\n",
    "        for j in i.get('openie_final'):\n",
    "            for k in j[1].split(' '):\n",
    "                if k=='associated' or  k=='associate' :\n",
    "                    bidirection=True\n",
    "                    break\n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "    i['bidirection']=bidirection\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "03635522-be98-43a4-aeee-84e5b9c193ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "abd25f1f-9107-4d20-8aca-edc1209d3a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "Relation_raw=copy.deepcopy(networkx_raw_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "e7148b0c-6e9b-438e-ae0f-4f46f70bdc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(networkx_raw_final,open('results/step3/networkx_raw_final.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "0a1e33d8-fb62-4a4c-81fa-8e9d32526989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PMID': '36071684',\n",
       " 'entity1': 'ERCC4',\n",
       " 'entity2': 'Atherosclerosis',\n",
       " 'sentence': 'Taken together, these findings indicated that lncRNA_FGF7-5 and lncRNA_GLRX3 together reduced atherosclerosis-induced apoptosis of HUVECs via targeting miR-2681-5p to increase ERCC4 expression, thereby preventing the formation of carotid plaque and finally inhibiting atherosclerosis progression.',\n",
       " 'AB': 'Atherosclerotic plaques belong to the common vascular disease in the aged, which rupture will lead to acute thromboembolic diseases, the leading cause of fatal cardiovascular events. Accumulating evidence indicates that the lncRNAs-miRNAs-mRNA regulatory network plays a critical role in atherosclerosis. Based on RNA sequencing (GSE207252), we constructed expression profiles of lncRNAs, microRNAs, and mRNA in the carotid plaque of atherosclerosis patients and analyzed differentially expressed genes (DEGs). We identified three candidate lncRNAs using two algorithms (LASSO and SVM-RFE): lnc_GLRX3, lnc_FGF7-5, and DISC1FP1). LNCipedia, TargetScan, and miRDB databases were used to predict target miRNAs of lncRNAs and target genes of miRNAs. Gene ontology (GO) functional annotation, Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway analysis, and Gene Set Enrichment Analysis (GSEA) analysis of DEGs was carried out using the R package clusterProfiler. A PPI network was constructed using the STRING website and visualized by Cytoscape. According to the \"MCC\" method of the plug-in cytoHubba in Cytoscape, ERCC4 was the top hub gene of the PPI network. We constructed a lncRNA_FGF7-5/lncRNA_GLRX3-miR-2681-5p-ERCC4 regulatory network for carotid plaque using lncRNA-miRNA and miRNA-mRNA pairs. Next, lncRNA_FGF7-5 and lncRNA_GLRX3 targeted miR-2681-5p directly to upregulate ERCC4 expression. Silencing of lncRNA_FGF7-5 and lncRNA_GLRX3 promoted apoptosis and TP53 expression in HUVECs treated with ox-LDL; however, these effects were reversed by ERCC4-overexpression. Taken together, these findings indicated that lncRNA_FGF7-5 and lncRNA_GLRX3 together reduced atherosclerosis-induced apoptosis of HUVECs via targeting miR-2681-5p to increase ERCC4 expression, thereby preventing the formation of carotid plaque and finally inhibiting atherosclerosis progression.',\n",
       " 'target1': 'ERCC4',\n",
       " 'target1_type': 'Gene',\n",
       " 'target2': 'atherosclerosis',\n",
       " 'target2_type': 'Disease',\n",
       " 'networkx': ['atherosclerosis', 'increase', 'ERCC4'],\n",
       " 'openie_final': [],\n",
       " 'allennlp_final': [],\n",
       " 'bidirection': False}"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "networkx_raw_final[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "022501c2-7525-4628-9129-fdfb5cae238d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in networkx_raw_final:\n",
    "    if i.get('openie_final') and i.get('openie_final')[0][1]=='highlights':\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c00a405-4a92-4b61-bc50-04ba198cf789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37e6726-f62e-410d-9cea-95979fada032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddb38f7-0676-42f3-8bac-1f8c5e330703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "d1b9b314-647d-4ff3-99d5-2b4693c42eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# networkx_raw_final=pickle.load(open('networkx_raw_final.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "fb582c18-111a-4960-8f1b-598c679e1d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2042"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(networkx_raw_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "1f9f4e4f-6656-4bfe-85b5-ec8f9d99adf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PMID': '36071684',\n",
       " 'entity1': 'ERCC4',\n",
       " 'entity2': 'Atherosclerosis',\n",
       " 'sentence': 'Taken together, these findings indicated that lncRNA_FGF7-5 and lncRNA_GLRX3 together reduced atherosclerosis-induced apoptosis of HUVECs via targeting miR-2681-5p to increase ERCC4 expression, thereby preventing the formation of carotid plaque and finally inhibiting atherosclerosis progression.',\n",
       " 'AB': 'Atherosclerotic plaques belong to the common vascular disease in the aged, which rupture will lead to acute thromboembolic diseases, the leading cause of fatal cardiovascular events. Accumulating evidence indicates that the lncRNAs-miRNAs-mRNA regulatory network plays a critical role in atherosclerosis. Based on RNA sequencing (GSE207252), we constructed expression profiles of lncRNAs, microRNAs, and mRNA in the carotid plaque of atherosclerosis patients and analyzed differentially expressed genes (DEGs). We identified three candidate lncRNAs using two algorithms (LASSO and SVM-RFE): lnc_GLRX3, lnc_FGF7-5, and DISC1FP1). LNCipedia, TargetScan, and miRDB databases were used to predict target miRNAs of lncRNAs and target genes of miRNAs. Gene ontology (GO) functional annotation, Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway analysis, and Gene Set Enrichment Analysis (GSEA) analysis of DEGs was carried out using the R package clusterProfiler. A PPI network was constructed using the STRING website and visualized by Cytoscape. According to the \"MCC\" method of the plug-in cytoHubba in Cytoscape, ERCC4 was the top hub gene of the PPI network. We constructed a lncRNA_FGF7-5/lncRNA_GLRX3-miR-2681-5p-ERCC4 regulatory network for carotid plaque using lncRNA-miRNA and miRNA-mRNA pairs. Next, lncRNA_FGF7-5 and lncRNA_GLRX3 targeted miR-2681-5p directly to upregulate ERCC4 expression. Silencing of lncRNA_FGF7-5 and lncRNA_GLRX3 promoted apoptosis and TP53 expression in HUVECs treated with ox-LDL; however, these effects were reversed by ERCC4-overexpression. Taken together, these findings indicated that lncRNA_FGF7-5 and lncRNA_GLRX3 together reduced atherosclerosis-induced apoptosis of HUVECs via targeting miR-2681-5p to increase ERCC4 expression, thereby preventing the formation of carotid plaque and finally inhibiting atherosclerosis progression.',\n",
       " 'target1': 'ERCC4',\n",
       " 'target1_type': 'Gene',\n",
       " 'target2': 'atherosclerosis',\n",
       " 'target2_type': 'Disease',\n",
       " 'networkx': ['atherosclerosis', 'increase', 'ERCC4'],\n",
       " 'openie_final': [],\n",
       " 'allennlp_final': [],\n",
       " 'bidirection': False}"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "networkx_raw_final[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "3cd9ecd0-b2d2-44c9-b3ca-1f3237152446",
   "metadata": {},
   "outputs": [],
   "source": [
    "collections_Relation_raw=db.Relation_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "e9168283-f0a4-4698-84d3-623753835610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023_03_14'"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c70613-69e6-418a-a420-516e1267b18b",
   "metadata": {},
   "source": [
    "collections_Relation_raw.remove({'addtime':now_time})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "66c2ba7d-2914-46da-9b1f-985516f3aee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Relation_raw:\n",
    "    i['addtime']=now_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "6a24efb6-72fd-4954-907a-aef6ec62e09b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x7f1e124c8c40>"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections_Relation_raw.insert_many(Relation_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a8bd5-b80a-4a42-beb1-1d4c01b99a72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d18f07f-b445-4f5f-abfb-b877bfd9ad63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fcc9b6-6f87-4002-a235-94c572fe69aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6657dd-e3e4-4a65-b0a9-2e6a0d83c13b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "e50321c8-e689-42cb-add9-8ac33c743cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Relation_raw_all_with_disease=copy.deepcopy(Relation_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "610a8457-beb3-4a73-8831-c10bcdec59c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2042"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Relation_raw_all_with_disease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "4a9e5856-7ccd-447d-ac1d-38cf0290e791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PMID': '36378468',\n",
       " 'entity1': 'MSI2',\n",
       " 'entity2': 'Memory Disorders',\n",
       " 'sentence': 'Furthermore, pharmacological inhibition of human MSI1 and MSI2 activity using (-)- gossypol resulted in improved memory retention, without causing locomotor, chemotactic, or learning deficits.',\n",
       " 'AB': \"Musashi RNA-binding proteins (MSIs) retain a pivotal role in stem cell maintenance, tumorigenesis, and nervous system development. Recently, we showed in C. elegans that Musashi (MSI-1) actively promotes forgetting upon associative learning via a 3'UTR-dependent translational expression of the Arp2/3 actin branching complex. Here, we investigated the evolutionary conserved role of MSI proteins and the effect of their pharmacological inhibition on memory. Expression of human Musashi 1 (MSI1) and Musashi 2 (MSI2) under the endogenous Musashi promoter fully rescued the phenotype of msi-1(lf) worms. Furthermore, pharmacological inhibition of human MSI1 and MSI2 activity using (-)- gossypol resulted in improved memory retention, without causing locomotor, chemotactic, or learning deficits. No drug effect was observed in msi-1(lf) treated worms. Using Western blotting and confocal microscopy, we found no changes in MSI-1 protein abundance following (-)- gossypol treatment, suggesting that Musashi gene expression remains unaltered and that the compound exerts its inhibitory effect post-translationally. Additionally, (-)- gossypol suppressed the previously seen rescue of the msi-1(lf) phenotype in worms expressing human MSI1 specifically in the AVA neuron, indicating that (-)- gossypol can regulate the Musashi pathway in a memory-related neuronal circuit in worms. Finally, treating aged worms with (-)- gossypol reversed physiological age-dependent memory decline. Taken together, our findings indicate that pharmacological inhibition of Musashi might represent a promising approach for memory modulation.\",\n",
       " 'target1': 'MSI2',\n",
       " 'target1_type': 'Gene',\n",
       " 'target2': 'memory retention',\n",
       " 'target2_type': 'Disease',\n",
       " 'allennlp': [['MSI2', 'resulted', 'memory retention']],\n",
       " 'networkx': ['MSI2', 'result', 'memory retention'],\n",
       " 'openie_final': [],\n",
       " 'allennlp_final': [['MSI2', 'resulted', 'memory retention']],\n",
       " 'bidirection': False,\n",
       " 'addtime': '2023_03_14',\n",
       " '_id': ObjectId('6411a00fee32117b2d8b6bfa')}"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Relation_raw_all_with_disease[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "2d9ac433-5003-42d6-92c4-94fb68b39096",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_es=json.load(open('results/step2/all_es_lack.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "16c8ffeb-aae6-43e4-b08c-ef868c8b7148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi_cheicals=[]\n",
    "multi_chemical_type2={}\n",
    "for i in all_es:\n",
    "    if not multi_chemical_type2.get(i.get('entity')):\n",
    "        multi_chemical_type2[i.get('entity')]=[i.get('type')]\n",
    "    else:\n",
    "        if i.get('type') not in multi_chemical_type2.get(i.get('entity')):\n",
    "            multi_chemical_type2[i.get('entity')]+=[i.get('type')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "162511e2-2851-49f1-aefa-4d951198f483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Lipopolysaccharides': ['Carbohydrate', 'Lipid', 'Toxin']}\n",
      "{'Vancomycin': ['Carbohydrate', 'Peptide']}\n",
      "{'Ceramides': ['Carbohydrate', 'Lipid']}\n",
      "{'Sphingomyelins': ['Carbohydrate', 'Lipid']}\n",
      "{'Bleomycin': ['Carbohydrate', 'Peptide']}\n",
      "{'Teicoplanin': ['Carbohydrate', 'Peptide']}\n",
      "{'Lipid A': ['Carbohydrate', 'Lipid', 'Toxin']}\n",
      "{'G(M1) Ganglioside': ['Carbohydrate', 'Lipid']}\n",
      "{'Gangliosides': ['Carbohydrate', 'Lipid']}\n",
      "{'Glycosphingolipids': ['Carbohydrate', 'Lipid']}\n",
      "{'Sulfoglycosphingolipids': ['Carbohydrate', 'Lipid']}\n",
      "{'Glycolipids': ['Carbohydrate', 'Lipid']}\n",
      "{'G(M3) Ganglioside': ['Carbohydrate', 'Lipid']}\n",
      "{'Glycopeptides': ['Carbohydrate', 'Peptide']}\n",
      "{'Glycosylphosphatidylinositols': ['Carbohydrate', 'Lipid']}\n",
      "{'Globosides': ['Carbohydrate', 'Lipid']}\n",
      "{'Ristocetin': ['Carbohydrate', 'Peptide']}\n",
      "{'Glucosylceramides': ['Carbohydrate', 'Lipid']}\n",
      "{'Holothurin': ['Carbohydrate', 'Toxin']}\n",
      "{'Glycerylphosphorylcholine': ['Carbohydrate', 'Lipid']}\n",
      "{'Glycerophosphates': ['Carbohydrate', 'Lipid']}\n",
      "{'Cerebrosides': ['Carbohydrate', 'Lipid']}\n",
      "{'Psychosine': ['Carbohydrate', 'Lipid']}\n",
      "{'Lactosylceramides': ['Carbohydrate', 'Lipid']}\n",
      "{'Phospholipid Ethers': ['Carbohydrate', 'Lipid']}\n",
      "{'Galactolipids': ['Carbohydrate', 'Lipid']}\n",
      "{'O Antigens': ['Carbohydrate', 'Lipid', 'Toxin']}\n",
      "{'Daptomycin': ['Lipid', 'Peptide']}\n",
      "{'Caspofungin': ['Lipid', 'Peptide']}\n",
      "{'Micafungin': ['Lipid', 'Peptide']}\n",
      "{'Lipopeptides': ['Lipid', 'Peptide']}\n",
      "{'Cilastatin, Imipenem Drug Combination': ['Lipid', 'Pharmaceutical Preparations']}\n",
      "{'Polymyxin B': ['Lipid', 'Peptide', 'Protein']}\n",
      "{'Exenatide': ['Peptide', 'Toxin']}\n",
      "{'Certolizumab Pegol': ['Peptide', 'Protein']}\n",
      "{'Phalloidine': ['Peptide', 'Toxin']}\n",
      "{'Abciximab': ['Peptide', 'Protein']}\n",
      "{'Alpha-Amanitin': ['Peptide', 'Toxin']}\n",
      "{'Charybdotoxin': ['Peptide', 'Toxin']}\n",
      "{'Angiotensins': ['Peptide', 'Protein']}\n",
      "{'Fas Ligand Protein': ['Peptide', 'Protein']}\n",
      "{'Arginine Vasopressin': ['Peptide', 'Protein']}\n",
      "{'Angiotensin II': ['Peptide', 'Protein']}\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "cxz=0\n",
    "for i,j in multi_chemical_type2.items():\n",
    "    if len(j)>1:\n",
    "        cxz+=1\n",
    "        print({i:j})\n",
    "print(cxz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "6d5d8a2a-9150-462d-b4a6-5b737ca91171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "Relation_raw_all_final=[]\n",
    "for i in Relation_raw_all_with_disease:\n",
    "    if multi_chemical_type2.get(i.get('entity1')) and multi_chemical_type2.get(i.get('entity2')):\n",
    "        for j in multi_chemical_type2.get(i.get('entity1')):\n",
    "            for k in multi_chemical_type2.get(i.get('entity2')):\n",
    "                mid=copy.deepcopy(i)\n",
    "                mid.update({'target1_type':j,'target2_type':k})\n",
    "                Relation_raw_all_final.append(mid)\n",
    "    else:\n",
    "        Relation_raw_all_final.append(i)\n",
    "        if multi_chemical_type2.get(i.get('entity1')):\n",
    "            for j in multi_chemical_type2.get(i.get('entity1')):\n",
    "                if i.get('target1_type') != j:\n",
    "                    mid=copy.deepcopy(i)\n",
    "                    mid.update({'target1_type':j})\n",
    "                    Relation_raw_all_final.append(mid)\n",
    "        elif multi_chemical_type2.get(i.get('entity2')):\n",
    "            for j in multi_chemical_type2.get(i.get('entity2')):\n",
    "                if i.get('target2_type') != j:\n",
    "                    mid=copy.deepcopy(i)\n",
    "                    mid.update({'target2_type':j})\n",
    "                    Relation_raw_all_final.append(mid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "7b98085f-ab87-4de8-b38d-c6bca56c6c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2074"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#122534\n",
    "len(Relation_raw_all_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "dbbdc1cd-3965-40da-b2be-151491d499eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "direction=set()\n",
    "direction_list=[]\n",
    "direction_type=[]\n",
    "for i in Relation_raw_all_final:\n",
    "\n",
    "    if i.get('openie_final'):\n",
    "        if i.get('openie_final')[0][0]!=i.get('target1'):\n",
    "            i['entity2'],i['entity1']=i['entity1'],i['entity2']\n",
    "            i['target2_type'],i['target1_type']=i['target1_type'],i['target2_type']\n",
    "            i['target2'],i['target1']=i['target1'],i['target2']\n",
    "    elif i.get('networkx'):\n",
    "        if i.get('networkx')[0]!= i.get('target1'):\n",
    "            i['entity2'],i['entity1']=i['entity1'],i['entity2']\n",
    "            i['target2_type'],i['target1_type']=i['target1_type'],i['target2_type']\n",
    "            i['target2'],i['target1']=i['target1'],i['target2']\n",
    "    elif i.get('allennlp_final'):\n",
    "        if i.get('allennlp_final')[0][0]!=i.get('target1'):\n",
    "            i['entity2'],i['entity1']=i['entity1'],i['entity2']\n",
    "            i['target2_type'],i['target1_type']=i['target1_type'],i['target2_type']\n",
    "            i['target2'],i['target1']=i['target1'],i['target2']\n",
    "    if i.get('bidirection'):\n",
    "        if (i.get('entity2'),(i.get('entity1'))) not in direction :\n",
    "            direction.add((i.get('entity2'),(i.get('entity1'))))\n",
    "            direction_list.append((i.get('entity2'),(i.get('entity1'))))\n",
    "            direction_type.append((i.get('target2_type'),(i.get('target1_type'))))\n",
    "        if (i.get('entity1'),(i.get('entity2'))) not in direction:\n",
    "            direction.add((i.get('entity1'),(i.get('entity2'))))\n",
    "            direction_list.append((i.get('entity1'),(i.get('entity2'))))\n",
    "            direction_type.append((i.get('target1_type'),(i.get('target2_type'))))\n",
    "    else:\n",
    "        direction.add((i.get('entity1'),(i.get('entity2'))))\n",
    "        direction_list.append((i.get('entity1'),(i.get('entity2'))))\n",
    "        direction_type.append((i.get('target1_type'),(i.get('target2_type'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "9e63337c-0157-48e6-b644-c6662bbaacd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PMID': '36378468',\n",
       " 'entity1': 'MSI2',\n",
       " 'entity2': 'Memory Disorders',\n",
       " 'sentence': 'Furthermore, pharmacological inhibition of human MSI1 and MSI2 activity using (-)- gossypol resulted in improved memory retention, without causing locomotor, chemotactic, or learning deficits.',\n",
       " 'AB': \"Musashi RNA-binding proteins (MSIs) retain a pivotal role in stem cell maintenance, tumorigenesis, and nervous system development. Recently, we showed in C. elegans that Musashi (MSI-1) actively promotes forgetting upon associative learning via a 3'UTR-dependent translational expression of the Arp2/3 actin branching complex. Here, we investigated the evolutionary conserved role of MSI proteins and the effect of their pharmacological inhibition on memory. Expression of human Musashi 1 (MSI1) and Musashi 2 (MSI2) under the endogenous Musashi promoter fully rescued the phenotype of msi-1(lf) worms. Furthermore, pharmacological inhibition of human MSI1 and MSI2 activity using (-)- gossypol resulted in improved memory retention, without causing locomotor, chemotactic, or learning deficits. No drug effect was observed in msi-1(lf) treated worms. Using Western blotting and confocal microscopy, we found no changes in MSI-1 protein abundance following (-)- gossypol treatment, suggesting that Musashi gene expression remains unaltered and that the compound exerts its inhibitory effect post-translationally. Additionally, (-)- gossypol suppressed the previously seen rescue of the msi-1(lf) phenotype in worms expressing human MSI1 specifically in the AVA neuron, indicating that (-)- gossypol can regulate the Musashi pathway in a memory-related neuronal circuit in worms. Finally, treating aged worms with (-)- gossypol reversed physiological age-dependent memory decline. Taken together, our findings indicate that pharmacological inhibition of Musashi might represent a promising approach for memory modulation.\",\n",
       " 'target1': 'MSI2',\n",
       " 'target1_type': 'Gene',\n",
       " 'target2': 'memory retention',\n",
       " 'target2_type': 'Disease',\n",
       " 'allennlp': [['MSI2', 'resulted', 'memory retention']],\n",
       " 'networkx': ['MSI2', 'result', 'memory retention'],\n",
       " 'openie_final': [],\n",
       " 'allennlp_final': [['MSI2', 'resulted', 'memory retention']],\n",
       " 'bidirection': False,\n",
       " 'addtime': '2023_03_14',\n",
       " '_id': ObjectId('6411a00fee32117b2d8b6bfa')}"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Relation_raw_all_final[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "b3f5ef60-52d2-4b96-9f67-8edce45001ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gene',\n",
       " 'RNA',\n",
       " 'Carbohydrate',\n",
       " 'Lipid',\n",
       " 'Peptide',\n",
       " 'Pharmaceutical_Preparations',\n",
       " 'Protein',\n",
       " 'Toxin',\n",
       " 'Mutation',\n",
       " 'Disease']"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "7d2e7a43-36d5-4e43-b76e-482138337b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/hald_autoupdate/2023_03_13'"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "5ec24314-4d22-459d-bfee-e1e86a273280",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_pairs=['sentence2adv','sentence2relations','sentence2verblist','sentence2verbprototype','sentence2words']\n",
    "names=locals()\n",
    "for i in rules_pairs:\n",
    "    names[i]={}\n",
    "    for k in ['results/step3/rules/files/'+i+str(j)+'.pkl' for j in range(1,22)]:\n",
    "        names[i].update(pickle.load(open(k,'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "7a7a3828-4863-4b34-bb7a-0b7e70efa050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "Relation_raw_all_final_copy=copy.deepcopy(Relation_raw_all_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "67e82c12-963a-4199-a826-567595590a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "be=['be','am','is','are','was','were','have been','has been']\n",
    "bedict={}\n",
    "for i in be:\n",
    "    bedict[i]=i\n",
    "can=['can','could','will','would','may','might']\n",
    "candict={}\n",
    "for i in can:\n",
    "    candict[i]=i\n",
    "countt=0\n",
    "counttt=0\n",
    "for i in Relation_raw_all_final_copy:\n",
    "    verblist=sentence2verblist.get(i.get('sentence'))\n",
    "    verbrelation=''\n",
    "    real_verb=''\n",
    "    verbdict={}\n",
    "    for verb in verblist:\n",
    "        verbdict.update(verb)\n",
    "    for indexx,j in enumerate(i.get('allennlp_final')):\n",
    "        one=j[1]\n",
    "        if verbdict.get(one):\n",
    "            verbrelation=one\n",
    "            relations=sentence2relations.get(i.get('sentence'))\n",
    "            relations_display=[relation[-2] for relation in relations]\n",
    "            verbprototype=sentence2verbprototype.get(i.get('sentence'))\n",
    "            verbprototype_dict={}\n",
    "            for verbp in verbprototype:\n",
    "                verbprototype_dict.update(verbp)\n",
    "            for index_passive,passive in enumerate(relations_display):\n",
    "                if passive=='aux:pass':\n",
    "                    if relations[index_passive][1]==verbrelation:\n",
    "                        verb_prototype=verbrelation\n",
    "\n",
    "                        break\n",
    "                elif passive=='acl' and relations[index_passive][-1]=='VBN':\n",
    "                    if relations[index_passive][0]==verbrelation:\n",
    "                        verb_prototype=verbrelation\n",
    "\n",
    "                        break\n",
    "            else:\n",
    "                for v in verbprototype:\n",
    "                    if v.get(verbrelation):\n",
    "                        verb_prototype=v.get(verbrelation)\n",
    "                        break\n",
    "            if verb_prototype!=one:\n",
    "                if countt<10:\n",
    "                    countt+=1\n",
    "                    # print(i.get('allennlp_final'))\n",
    "                    # print(i.get('sentence'))\n",
    "                    # print(verb_prototype)\n",
    "            i['allennlp_final'][indexx][1]=verb_prototype\n",
    "        elif bedict.get(one):\n",
    "            i['allennlp_final'][indexx][1]=bedict.get(one)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e643cbd-7c61-43ae-9f4f-a5025f680e3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "drop_all1=[]\n",
    "drop_all2=[]\n",
    "for i in Relation_raw_all_final_copy:\n",
    "    sentence2verbdict={}\n",
    "    for j in sentence2verblist.get(i.get('sentence')):\n",
    "        sentence2verbdict[list(j.keys())[0]]=list(j.keys())[0]\n",
    "    for j in sentence2verbprototype.get(i.get('sentence')):\n",
    "        sentence2verbdict[list(j.values())[0]]=list(j.values())[0]\n",
    "    sentence2verbdict.update(bedict)\n",
    "    if i.get('openie_final'):\n",
    "        openie_final=[]\n",
    "        for j in i.get('openie_final'):\n",
    "            openi=False\n",
    "            for k in j[1].split(' '):\n",
    "                if sentence2verbdict.get(k):\n",
    "                    openi=True\n",
    "                    break\n",
    "            if openi:\n",
    "                openie_final.append(j)\n",
    "            else:\n",
    "                drop_all1.append(j)\n",
    "        # if len(openie_final) != len(i.get('openie_final')):\n",
    "        #     drop_all1.append(openie_final)\n",
    "        #     drop_all2.append(i.get('openie_final'))\n",
    "        i['openie_final']=openie_final\n",
    "#     if i.get('allennlp_final'):\n",
    "#         allennlp_final=[]\n",
    "#         for j in i.get('allennlp_final'):\n",
    "#             allennl=False\n",
    "#             for k in j[1].split(' '):\n",
    "#                 if sentence2verbdict.get(k):\n",
    "#                     allennl=True\n",
    "#                     break\n",
    "#             if allennl:\n",
    "#                 allennlp_final.append(j)\n",
    "#             else:\n",
    "#                 drop_all2.append(j)\n",
    "#         # if len(allennlp_final) != len(i.get('allennlp_final')):\n",
    "#         #     drop_all1.append(allennlp_final)\n",
    "#         #     drop_all2.append(i.get('allennlp_final'))\n",
    "#         i['allennlp_final']=allennlp_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "ba78bc16-be4e-4309-87c7-ddbb4604350e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2074"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Relation_raw_all_final_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "e43e3130-8533-4e97-aea2-b79a86b692fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Relation_raw_all_final_copy:\n",
    "    if i.get('_id'):\n",
    "        del i['_id']\n",
    "json.dump(Relation_raw_all_final_copy,open('results/step3/triplelist.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "c1720849-b1e5-4c10-a46a-3b75182753ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2074"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Relation_raw_all_final_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "8feefe62-f204-4783-b9ae-767766e0c0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "collections_Relation_final=db.Relation_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95dfa36-1c09-4fc6-b4a0-0ecfecad48a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7d6c740-5ad3-43b4-b1db-1ca6f575c8ed",
   "metadata": {},
   "source": [
    "collections_Relation_final.remove({'addtime':now_time})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "7e3b078f-ed9a-4ebb-bd83-b7bca09bc68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Relation_raw_all_final_copy:\n",
    "    i['addtime']=now_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "f90385cb-d04e-46c1-b90e-6a79e3f04d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x7f2f4befc640>"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections_Relation_final.insert_many(Relation_raw_all_final_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "4f38ea86-783d-4b10-9f82-8a55f7fc15c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(Relation_raw_all_final_copy,open('results/step3/Relation_raw_all_final_copy.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "dfddd52e-0732-40fb-afdd-97e0ed5f5b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(Relation_raw,open('results/step3/Relation_raw.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "362ac820-cf8f-4b59-9dbb-9b69b6b451c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/hald_autoupdate/2023_03_13'"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10d220d-1499-4eef-87db-6e0d22103fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "d2780d70-8c2d-4e14-8b54-6802e606c275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'means': 'VBZ'}]"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence2verblist.get('Corneal rupture at the graft-host junction in all of our cases means the persistence of wound weakness after PK.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "2e488714-88fa-42e9-8989-addff641d9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'means': 'means'}]"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence2verbprototype.get('Corneal rupture at the graft-host junction in all of our cases means the persistence of wound weakness after PK.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040e441f-a7f8-4c21-8f44-d6e62d59b899",
   "metadata": {},
   "source": [
    "rules_pairs=['sentence2adv','sentence2relations','sentence2verblist','sentence2verbprototype','sentence2words']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef23ce6-bd85-481b-aa2c-d6a1f584cbd8",
   "metadata": {},
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6103a1b8-dd85-4824-ac88-679dc24c2bd0",
   "metadata": {},
   "source": [
    "for i in rules_pairs:\n",
    "    names[i]={}\n",
    "    for k in ['results/step3/rules/files/'+i+str(j)+'.pkl' for j in range(1,22)]:\n",
    "        names[i].update(pickle.load(open(k,'rb')))\n",
    "    for k in ['../2022_12_09/rules/files/'+i+str(j)+'.pkl' for j in range(1,22)]:\n",
    "        names[i].update(pickle.load(open(k,'rb')))\n",
    "    names[i].update(pickle.load(open('../2022_12_09/rules/files/'+i+'_old.pkl','rb')))\n",
    "    # pickle.dump(names[i],open('files/'+i+'.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a00885-e4ac-43b6-892b-e3dccb243fd5",
   "metadata": {},
   "source": [
    "drop_all1=[]\n",
    "drop_all2=[]\n",
    "for i in Relation_raw_all_final_copy_all:\n",
    "    sentence2verbdict={}\n",
    "    for j in sentence2verblist.get(i.get('sentence')):\n",
    "        sentence2verbdict[list(j.keys())[0]]=list(j.keys())[0]\n",
    "    for j in sentence2verbprototype.get(i.get('sentence')):\n",
    "        sentence2verbdict[list(j.values())[0]]=list(j.values())[0]\n",
    "    sentence2verbdict.update(bedict)\n",
    "    if i.get('openie_final'):\n",
    "        openie_final=[]\n",
    "        for j in i.get('openie_final'):\n",
    "            openi=False\n",
    "            for k in j[1].split(' '):\n",
    "                if sentence2verbdict.get(k):\n",
    "                    openi=True\n",
    "                    break\n",
    "            if openi:\n",
    "                openie_final.append(j)\n",
    "            else:\n",
    "                drop_all1.append(j)\n",
    "        # if len(openie_final) != len(i.get('openie_final')):\n",
    "        #     drop_all1.append(openie_final)\n",
    "        #     drop_all2.append(i.get('openie_final'))\n",
    "        i['openie_final']=openie_final\n",
    "#     if i.get('allennlp_final'):\n",
    "#         allennlp_final=[]\n",
    "#         for j in i.get('allennlp_final'):\n",
    "#             allennl=False\n",
    "#             for k in j[1].split(' '):\n",
    "#                 if sentence2verbdict.get(k):\n",
    "#                     allennl=True\n",
    "#                     break\n",
    "#             if allennl:\n",
    "#                 allennlp_final.append(j)\n",
    "#             else:\n",
    "#                 drop_all2.append(j)\n",
    "#         # if len(allennlp_final) != len(i.get('allennlp_final')):\n",
    "#         #     drop_all1.append(allennlp_final)\n",
    "#         #     drop_all2.append(i.get('allennlp_final'))\n",
    "#         i['allennlp_final']=allennlp_final\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768b85d5-97b7-4228-b14f-6412f1ea2c55",
   "metadata": {},
   "source": [
    "len(drop_all1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42710c8c-7f85-475f-9a3d-dc98c9c7f7e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hald",
   "language": "python",
   "name": "hald"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
